{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMn/EvzF0wgBNsMti2CAmr4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pvst07/Fake-Talk-Detector-Project/blob/main/deploy_gradio_to_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6ecybt4azy8G"
      },
      "outputs": [],
      "source": [
        "!pip install gradio huggingface_hub torchvision librosa moviepy pydub matplotlib Pillow\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"requirements.txt\", \"w\") as f:\n",
        "    f.write(\"\"\"\n",
        "gradio\n",
        "torch\n",
        "torchvision\n",
        "librosa\n",
        "moviepy\n",
        "pydub\n",
        "matplotlib\n",
        "Pillow\n",
        "ffmpeg-python\n",
        "numpy\n",
        "scipy\n",
        "soundfile\n",
        "\n",
        "    \"\"\")"
      ],
      "metadata": {
        "id": "4soiaKwT3TUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = '''\n",
        "import os\n",
        "os.system(\"apt-get update && apt-get install -y ffmpeg\")\n",
        "\n",
        "import gradio as gr\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import librosa\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import io\n",
        "import tempfile\n",
        "from pydub import AudioSegment\n",
        "\n",
        "def extract_audio_from_video(video_path):\n",
        "    from moviepy.editor import VideoFileClip  # <--- lazy import\n",
        "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmp_audio_file:\n",
        "        audio_path = tmp_audio_file.name\n",
        "    clip = VideoFileClip(video_path)\n",
        "    clip.audio.write_audiofile(audio_path, codec='pcm_s16le', fps=16000, verbose=False, logger=None)\n",
        "    return audio_path\n",
        "\n",
        "def get_resnet34_model():\n",
        "    model = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n",
        "    model.fc = nn.Sequential(\n",
        "        nn.Linear(model.fc.in_features, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256, 64),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(64, 2)\n",
        "    )\n",
        "    return model\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = get_resnet34_model().to(device)\n",
        "model.load_state_dict(torch.load(\"final_model.pt\", map_location=device))\n",
        "model.eval()\n",
        "\n",
        "\n",
        "def get_loudest_segment(audio_path, sr=16000, window_size=5):\n",
        "    audio, _ = librosa.load(audio_path, sr=sr)\n",
        "    duration = len(audio) / sr\n",
        "    window_len = int(window_size * sr)\n",
        "    if len(audio) < window_len:\n",
        "        return 0, duration\n",
        "    max_energy = 0\n",
        "    best_start = 0\n",
        "    for start in range(0, len(audio) - window_len, sr):\n",
        "        end = start + window_len\n",
        "        energy = np.sum(audio[start:end] ** 2)\n",
        "        if energy > max_energy:\n",
        "            max_energy = energy\n",
        "            best_start = start\n",
        "    return best_start / sr, (best_start + window_len) / sr\n",
        "\n",
        "def crop_audio_segment(audio_path, start_sec, end_sec):\n",
        "    audio = AudioSegment.from_file(audio_path).set_frame_rate(16000).set_channels(1)\n",
        "    cropped = audio[start_sec * 1000:end_sec * 1000]\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as tmp:\n",
        "        cropped.export(tmp.name, format=\"wav\")\n",
        "        return tmp.name\n",
        "\n",
        "def audio_to_melspectrogram(audio_path, sr=16000, n_mels=128, fmax=8000):\n",
        "    y, _ = librosa.load(audio_path, sr=sr)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels, fmax=fmax)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    mel_norm = (mel_db - mel_db.min()) / (mel_db.max() - mel_db.min())\n",
        "    mel_img = (mel_norm * 255).astype(np.uint8)\n",
        "    return mel_img\n",
        "\n",
        "def preprocess_mel(mel_img):\n",
        "    img = Image.fromarray(mel_img).convert(\"RGB\").resize((224, 224))\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    return transform(img).unsqueeze(0)\n",
        "\n",
        "def predict_audio_file(audio_path):\n",
        "    mel_img = audio_to_melspectrogram(audio_path)\n",
        "    input_tensor = preprocess_mel(mel_img).to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        probs = torch.softmax(outputs, dim=1).cpu().numpy()[0]\n",
        "    classes = [\"Real\", \"Fake\"]\n",
        "    pred_idx = np.argmax(probs)\n",
        "    pred_label = classes[pred_idx]\n",
        "    confidence = round(probs[pred_idx] * 100, 2)\n",
        "    buf = io.BytesIO()\n",
        "    plt.figure(figsize=(4, 4))\n",
        "    plt.imshow(mel_img, cmap='magma', origin='lower')\n",
        "    plt.axis('off')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(buf, format='png')\n",
        "    plt.close()\n",
        "    buf.seek(0)\n",
        "    return pred_label, confidence, Image.open(buf)\n",
        "\n",
        "def run_full_predict(audio_file):\n",
        "    if audio_file is None:\n",
        "        return None, \"<b style='color:red;'>‚ùå No file</b>\", \"\", None\n",
        "    ext = os.path.splitext(audio_file)[-1].lower()\n",
        "    is_video = ext in [\".mp4\", \".mov\", \".avi\", \".mkv\"]\n",
        "    extracted_path = extract_audio_from_video(audio_file) if is_video else audio_file\n",
        "    duration = librosa.get_duration(filename=extracted_path)\n",
        "    if duration > 5.1:\n",
        "        start_sec, end_sec = get_loudest_segment(extracted_path)\n",
        "        cropped_path = crop_audio_segment(extracted_path, start_sec, end_sec)\n",
        "        trim_info = f\"<div style='color:gray;'>‚è±Ô∏è Auto-trimmed to {start_sec:.2f}‚Äì{end_sec:.2f} sec</div>\"\n",
        "    else:\n",
        "        cropped_path = extracted_path\n",
        "        trim_info = \"\"\n",
        "    label, conf, mel_img = predict_audio_file(cropped_path)\n",
        "    label_color = \"green\" if label == \"Real\" else \"red\"\n",
        "    html_label = f\"<div style='font-size:28px; font-weight:bold; color:{label_color};'>{label}</div>\"\n",
        "    html_conf = f\"<div style='font-size:28px; font-weight:bold;'>{conf:.2f}%</div>\"\n",
        "    final_html = f\"<div style='display:flex; justify-content:center; gap:40px;'>{html_label}{html_conf}</div>{trim_info}\"\n",
        "    return cropped_path, final_html, mel_img\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## FakeTalk Detector\")\n",
        "    with gr.Row():\n",
        "      file_input = gr.File(label=\"üéµ Upload Audio/Video\", file_types=[\".wav\", \".mp3\", \".mp4\", \".mov\", \".avi\", \".mkv\"])\n",
        "    with gr.Row():\n",
        "        trimmed_audio = gr.Audio(label=\"üîä Trimmed 5s Preview\", type=\"filepath\", interactive=False)\n",
        "    with gr.Row():\n",
        "        submit_btn = gr.Button(\"Submit for Analysis\", variant=\"primary\")\n",
        "    result_html = gr.HTML()\n",
        "    mel_output = gr.Image(type=\"pil\", label=\"Log Mel Spectrogram\")\n",
        "    def show_trimmed_audio(audio_file):\n",
        "        if audio_file is None:\n",
        "            return None\n",
        "        ext = os.path.splitext(audio_file)[-1].lower()\n",
        "        is_video = ext in [\".mp4\", \".mov\", \".avi\", \".mkv\"]\n",
        "        extracted_path = extract_audio_from_video(audio_file) if is_video else audio_file\n",
        "        duration = librosa.get_duration(filename=extracted_path)\n",
        "        if duration > 5.1:\n",
        "            start_sec, end_sec = get_loudest_segment(extracted_path)\n",
        "            cropped_path = crop_audio_segment(extracted_path, start_sec, end_sec)\n",
        "        else:\n",
        "            cropped_path = extracted_path\n",
        "        return cropped_path\n",
        "\n",
        "    file_input.change(fn=show_trimmed_audio, inputs=file_input, outputs=trimmed_audio)\n",
        "    submit_btn.click(fn=run_full_predict, inputs=file_input, outputs=[trimmed_audio, result_html, mel_output])\n",
        "\n",
        "demo.launch()\n",
        "'''\n",
        "with open(\"app.py\", \"w\") as f:\n",
        "    f.write(code)"
      ],
      "metadata": {
        "id": "sGx_dzc70BMw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "eNYwScSz0kKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi()\n",
        "api.create_repo(\n",
        "    repo_id=\"pvs-tw/fake-talk-detector\",\n",
        "    repo_type=\"space\",\n",
        "    space_sdk=\"gradio\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "fM94PgLK8hLp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"final_model.pt\",\n",
        "    path_in_repo=\"final_model.pt\",\n",
        "    repo_id=\"pvs-tw/fake-talk-detector\",\n",
        "    repo_type=\"space\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ohzaw1yYINUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "repo_id = \"pvs-tw/fake-talk-detector\"\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"requirements.txt\",\n",
        "    path_in_repo=\"requirements.txt\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"space\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "TgqPZfSvEWhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api = HfApi()\n",
        "repo_id = \"pvs-tw/fake-talk-detector\"\n",
        "\n",
        "api.upload_file(\n",
        "    path_or_fileobj=\"app.py\",\n",
        "    path_in_repo=\"app.py\",\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"space\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Sj4DkRY05TEZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}